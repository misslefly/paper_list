| 阅读时间 | 名称                                                         | 简介                                                         | 笔记                                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 20200523 | [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583) | 文本分类任务上BERT微调实验                                   | [脑图](http://naotu.baidu.com/file/d5326fe5553fb22206d5687f598c88f3?token=eca20202e006c2b2) |
| 20200530 | [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/abs/1801.10198) | WikiSum数据集，two-stage extractive-abstractive framework，T-D(Transformer-Decoder), T-DMCA(Transformer- Decoder Memory-Compressed Attention) | [脑图](http://naotu.baidu.com/file/6ecc4419fe3454875d1b4168ed0b97bb?token=5699cb020d40f0b2) |
| 20200603 | [Recent advances in document summarization](https://link.springer.com/article/10.1007/s10115-017-1042-4) | 17年万小军团队摘要survey                                     | [脑图](https://naotu.baidu.com/file/332964e4baf4955d619bc630ea05895a) |
| 20200604 | [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/abs/1906.01749) | 多文档摘要数据集                                             | [脑图][https://naotu.baidu.com/file/28a8ecdcdf275d4267c48a6795d8b26d] |
| 20200605 | [Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization](https://www.aclweb.org/anthology/D18-1446/) | PG-MMR，将预训练单文档摘要模型迁移至多文档摘要，使用MMR选择重要模型 | [脑图](https://naotu.baidu.com/file/777e0b2a7357943fa8b3000d6ca265f9) |
| 20200606 | [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368) | Pointer-Generator，生成式摘要经典模型                        | [脑图](https://naotu.baidu.com/file/b59ec96c92bb97e0fc8edf36141c19e4) |
| 20200617 | [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) | 提出文本摘要的预训练任务：Gap Sentences Generation，并在下游12个任务上取得SOTA，具有零样本、小样本迁移能力 | [脑图](https://naotu.baidu.com/file/51a98e4faf6069846340eb084264b900) |

